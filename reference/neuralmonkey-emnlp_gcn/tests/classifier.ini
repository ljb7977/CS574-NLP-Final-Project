;; small classification test

[main]
name="classification"
tf_manager=<tf_manager>
output="tests/outputs/classifier"
overwrite_output_dir=True
batch_size=16
epochs=4
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>]
postprocess=None
evaluation=[("classification", evaluators.accuracy.Accuracy)]
logging_period=50
validation_period=100
runners_batch_size=1
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1

[train_data]
class=dataset.load_dataset_from_files
s_source="tests/data/train.tc.en"
s_classification="tests/data/train.words"
preprocessors=[("source", "source_chars", processors.helpers.preprocess_char_based)]
lazy=True

[val_data]
class=dataset.load_dataset_from_files
s_source="tests/data/val.tc.en"
s_classification="tests/data/val.words"
preprocessors=[("source", "source_chars", processors.helpers.preprocess_char_based)]

[encoder_vocabulary]
class=vocabulary.from_file
path="tests/outputs/vocab/encoder_vocab.pkl"

[encoder_rnn]
class=encoders.sentence_encoder.SentenceEncoder
name="sentence_encoder"
rnn_size=7
max_input_len=4
embedding_size=11
dropout_keep_prob=0.5
attention_type=decoding_function.Attention
data_id="source"
vocabulary=<encoder_vocabulary>

[encoder_cnn]
class=encoders.sequence_cnn_encoder.SequenceCNNEncoder
name="sentence_cnn"
filters=[(2, 3), (3, 4)]
max_input_len=4
embedding_size=11
dropout_keep_prob=0.5
data_id="source"
vocabulary=<encoder_vocabulary>

[decoder_vocabulary]
class=vocabulary.from_wordlist
path="tests/data/classification.vocab"

[decoder]
class=decoders.sequence_classifier.SequenceClassifier
name="decoder"
encoders=[<encoder_rnn>, <encoder_cnn>]
dropout_keep_prob=0.5
layers=[10,5]
data_id="classification"
activation_fn=tf.nn.relu
vocabulary=<decoder_vocabulary>

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-8
clip_norm=1.0

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="classification"
