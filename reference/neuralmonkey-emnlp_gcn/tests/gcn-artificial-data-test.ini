; Run artificial data GCN experiment

[main]
name="translation"
tf_manager=<tf_manager>
output="tests/tmp-gcn-artificial"
overwrite_output_dir=True
batch_size=80
epochs=10
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>]
postprocess=None
evaluation=[("target", evaluators.ter.TER), ("target", <bleu>)]
logging_period=20
validation_period=313
runners_batch_size=80
random_seed=1234
val_preview_num_examples=3

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1
minimize_metric=False

[bleu]
class=evaluators.bleu.BLEUEvaluator

[train_data]
class=dataset.load_dataset_from_files
s_source="tests/data/gcn_artificial/train.scrambled.words"
s_deprels="tests/data/gcn_artificial/train.scrambled.labels"
s_heads="tests/data/gcn_artificial/train.scrambled.heads"
s_fakedeprels="tests/data/gcn_artificial/train.fake.labels"
s_fakeheads="tests/data/gcn_artificial/train.fake.heads"
s_target="tests/data/gcn_artificial/train.words"

[val_data]
class=dataset.load_dataset_from_files
name="validation"
s_source="tests/data/gcn_artificial/dev.scrambled.words"
s_deprels="tests/data/gcn_artificial/dev.scrambled.labels"
s_heads="tests/data/gcn_artificial/dev.scrambled.heads"
s_fakedeprels="tests/data/gcn_artificial/dev.fake.labels"
s_fakeheads="tests/data/gcn_artificial/dev.fake.heads"
s_target="tests/data/gcn_artificial/dev.words"

[source_word_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
max_size=26
series_ids=["source"]
save_file="tests/tmp-gcn-artificial/source_word_vocabulary.pkl"
overwrite=True
;class=vocabulary.from_file
;path="tests/tmp-gcn-artificial/source_word_vocabulary.pkl"

[deprel_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
max_size=100
series_ids=["deprels", "fakedeprels"]
save_file="tests/tmp-gcn-artificial/deprel_vocabulary.pkl"
overwrite=True
;class=vocabulary.from_file
;path="tests/tmp-gcn-artificial/deprel_vocabulary.pkl"

[encoder]
class=encoders.sentence_encoder.SentenceEncoder
name="sentence_encoder"
rnn_size=64
max_input_len=25
embedding_size=32
dropout_keep_prob=0.8
attention_type=decoding_function.Attention
data_id="source"
vocabulary=<source_word_vocabulary>
add_start_symbol=True

[gcn_encoder]
class=encoders.gcn_encoder.GCNEncoder
name="gcn_encoder"
layer_size=128
max_input_len=25
dropout_keep_prob=0.8
edge_dropout_keep_prob=0.9
attention_type=decoding_function.Attention
data_ids=["source", "deprels", "heads", "fakedeprels", "fakeheads"]
vocabularies=[<source_word_vocabulary>, <deprel_vocabulary>, <deprel_vocabulary>]
parent_encoder=<encoder>

[target_word_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
max_size=26
save_file="tests/tmp-gcn-artificial/target_word_vocabulary.pkl"
overwrite=True
series_ids=["target"]

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<gcn_encoder>]
rnn_size=64
embedding_size=32
use_attention=True
dropout_keep_prob=0.8
data_id="target"
max_output_len=25
vocabulary=<target_word_vocabulary>

[adam]
class=config.utils.adam_optimizer
learning_rate=1.0e-3

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-8
clip_norm=1.0
optimizer=<adam>

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
