; ====================================
; FACTORED INPUT EXAMPLE CONFIGURATION
; ====================================

; This is an example configuration for the machine translation task
; with factored input. The factors added to the source data in this
; example are the part-of-speech tags.

; The FactoredEncoder class is a generalized implementation of an
; encoder with an arbitrary number of input data series.

[main]
name=translation with factored input
output=out-example-factored
overwrite_output_dir=True
batch_size=128
epochs=10
encoders=[<encoder>]
decoder=<decoder>
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runner=<runner>
postprocess=None
evaluation=[<bleu>]
logging_period=40
validation_period=1000

[bleu]
class=evaluators.bleu.BLEUEvaluator


; Training and validation datasets both have three data series:
; source, tags, and target. The first two are used as the input data,
; the target data series contains the reference translations used for
; training or validation respectively.

[train_data]
class=dataset.load_dataset_from_files
s_source=examples/data/pcedt2/train.forms-en.txt
s_tags=examples/data/pcedt2/train.tags-en.txt
s_target=examples/data/pcedt2/train.forms-cs.txt

[val_data]
class=dataset.load_dataset_from_files
s_source=examples/data/pcedt2/val.forms-en.txt
s_tags=examples/data/pcedt2/val.tags-en.txt
s_target=examples/data/pcedt2/val.forms-cs.txt


; As with the data series, we have to create three vocabularies (one
; for each data series). Here, the tag vocabulary is inferred from the
; dataset and restricted to contain at most 50 different tags.

[source_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[source]
max_size=50000

[target_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[target]
max_size=50000

[tags_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[tags]
max_size=50


; The configuration of the factored encoder is similar to the
; configuration of the sentence encoder, apart from the fact that some
; of the arguments are lists now. The data_ids is the list that
; specifies which data series should this encoder use. For each input
; data series, the embedding size and vocabulary is specified in the
; respective parameter setting.

[encoder]
class=encoders.factored_encoder.FactoredEncoder
rnn_size=256
max_input_len=50
dropout_keep_prob=0.8
attention_type=decoding_function.Attention

data_ids=[source, tags]
embedding_sizes=[300, 100]
vocabularies=[<source_vocabulary>, <tags_vocabulary>]


; The configuration of the decoder, trainer, and runner stays the same
; as in the classic translation task.

[decoder]
class=decoders.decoder.Decoder
encoders=[<encoder>]
rnn_size=256
embedding_size=256
use_attention=True
dropout_keep_prob=0.5
data_id=target
vocabulary=<target_vocabulary>

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoder=<decoder>
l2_regularization=1.0e-8

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
batch_size=256
