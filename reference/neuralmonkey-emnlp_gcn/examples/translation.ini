; This is an example configuration for training machine translation.  It is an
; INI file with few added syntactic restrictions.
;
; Names in square brackets refer to objects in the program. With the exception
; of the [main] block, all of them will be instantiated as objects.
;
; The field values can be of several types:
;
;   * None - interpreted as Python None
;   * True / False - interpreted as boolean values
;   * integers
;   * floating point numbers
;   * Python types (fully defined with module name)
;   * references to other objects in the configuration, closed in <>
;   * strings in quotes
;   * list of the previous, enclosed in square brackets, comma-separated
;


; The main block contains the mandatory fields for running an experiment.
; It is the only block that does not have the `class` parameter
[main]
name="translation"
output="out-example-translation"
tf_manager=<tf_manager>

train_dataset=<train_data>
val_dataset=<val_data>

runners=[<runner>]
trainer=<trainer>
evaluation=[("target", evaluators.bleu.BLEU1), ("target", evaluators.bleu.BLEU4)]

batch_size=128
runners_batch_size=128
epochs=10

validation_period=1000
logging_period=20


; The TF manager section configures TF flags and learning mode
; increasing number of session can be used for model ensembles
[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1


; Below are definitions of the training and validation data objects.  Dataset
; is not a standard class, it treats the __init__ method's arguments as a
; dictionary, therefore the data series names can be any string, prefixed with
; "s_". To specify the output file for a series, use "s_" prefix and "_out"
; suffix, e.g.  "s_target_out" Series-level preprocessors can be specified by
; prefixing the resulting series name by `pre_`. Dataset-level preprocessors
; are entered separately as the `preprocessors` parameter with triples `raw`,
; `preprocessed`, `preprocessor`.
[train_data]
class=dataset.load_dataset_from_files
s_source="examples/data/translation/train.en"
s_target="examples/data/translation/train.de"
lazy=True
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]

[val_data]
class=dataset.load_dataset_from_files
s_source="examples/data/translation/val.en"
s_target="examples/data/translation/val.de"
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]


; Definition of byte-pair encoding preprocessor and postprocessor.
[bpe_preprocess]
class=processors.bpe.BPEPreprocessor
merge_file="examples/data/translation/bpe_merges"

[bpe_postprocess]
class=processors.bpe.BPEPostprocessor


; Definition of the vocabulary. In this example, we use a shared vocabulary
; for both source and target language. As we are using BPE preprocessing,
; we can create the vocabulary from the BPE merge file
[shared_vocabulary]
class=vocabulary.from_bpe
path="examples/data/translation/bpe_merges"


; This section defines the sentence encoder object.
[encoder]
class=encoders.sentence_encoder.SentenceEncoder
name="sentence_encoder"
rnn_size=300
max_input_len=50
embedding_size=300
dropout_keep_prob=1.0
attention_type=decoding_function.Attention
data_id="source_bpe"
vocabulary=<shared_vocabulary>


; Decoder setting definition
[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
rnn_size=300
embedding_size=300
use_attention=True
dropout_keep_prob=1.0
data_id="target_bpe"
vocabulary=<shared_vocabulary>
max_output_len=50


; The definition of the trainer. The trainer is used to evaluating the
; optimization op of the TensorFlow graph.
[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-8
clip_norm=1.0


; The definition of the greedy runner. It computes the decoding operations
; described in the graph.
[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=<bpe_postprocess>